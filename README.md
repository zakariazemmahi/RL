# Q-Learning Adaptatif - Environnement Mobile GridWorld

## ğŸ“ Description

ImplÃ©mentation d'un agent Q-Learning adaptatif capable d'apprendre Ã  naviguer dans un environnement dynamique avec des objectifs (goals) et des obstacles mobiles. Ce projet dÃ©montre l'apprentissage par renforcement dans un contexte non-stationnaire.

## âœ¨ CaractÃ©ristiques

- **Environnement dynamique** : Goals et obstacles se dÃ©placent alÃ©atoirement
- **Q-Learning adaptatif** : L'agent apprend Ã  s'adapter aux changements
- **DÃ©tection de boucles** : Ã‰vite que l'agent tourne en rond indÃ©finiment
- **Visualisation en temps rÃ©el** : Animation des mouvements de l'agent
- **Analyse des performances** : Graphiques d'apprentissage et heatmaps
- **Configuration flexible** : ParamÃ¨tres manuels ou gÃ©nÃ©ration alÃ©atoire

## ğŸš€ Installation

### PrÃ©requis

- Python 3.7 ou supÃ©rieur
- pip (gestionnaire de packages Python)

### Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

## ğŸ’» Utilisation

### Lancer le programme

```bash
python main.py
```

### Options de configuration

Au dÃ©marrage, vous avez deux options :

1. **Configuration manuelle** : DÃ©finir manuellement la taille de la grille, positions des goals, obstacles, etc.
2. **GÃ©nÃ©ration alÃ©atoire** : Laisser le programme gÃ©nÃ©rer un environnement alÃ©atoire

### Exemple d'exÃ©cution

```
CONFIGURATION DE L'ENVIRONNEMENT MOBILE
Voulez-vous:
  1. Entrer les paramÃ¨tres manuellement
  2. GÃ©nÃ©rer alÃ©atoirement
Votre choix (1 ou 2): 2
```

## ğŸ“Š FonctionnalitÃ©s

### 1. EntraÃ®nement de l'agent
- Apprentissage par Q-Learning sur plusieurs Ã©pisodes
- Adaptation dynamique du taux d'exploration (epsilon)
- DÃ©tection automatique de stagnation

### 2. Visualisations
- **Animation en temps rÃ©el** : Mouvements de l'agent, goals et obstacles
- **Politique apprise** : Affichage des Q-values et directions optimales
- **Courbes d'apprentissage** :
  - Ã‰volution des rÃ©compenses
  - Nombre de pas par Ã©pisode
  - Taux de succÃ¨s
  - Distribution des rÃ©compenses
- **Heatmap** : Visualisation des Q-values moyennes par Ã©tat

### 3. Comparaison de stratÃ©gies
- Politique Q-Learning vs Politique alÃ©atoire
- Ã‰valuation sur plusieurs Ã©pisodes

## ğŸ§  Algorithme

### Q-Learning Adaptatif

L'algorithme utilise plusieurs techniques pour gÃ©rer l'environnement dynamique :

1. **Learning rate adaptatif** : DÃ©croÃ®t avec le nombre de visites
2. **Exploration intelligente** : Favorise les actions moins explorÃ©es
3. **Bonus d'exploration** : Encourage la dÃ©couverte de nouveaux Ã©tats
4. **DÃ©tection de boucles** : PÃ©nalise les comportements rÃ©pÃ©titifs
5. **RÃ©initialisation adaptative** : Augmente l'exploration si stagnation dÃ©tectÃ©e

### RÃ©compenses

- **+10** : Atteindre un goal
- **-0.1** : CoÃ»t de dÃ©placement normal
- **-1** : Collision avec un obstacle
- **-2** : DÃ©tection de boucle (tourner en rond)
- **-0.5** : Revisiter trop souvent le mÃªme Ã©tat

## ğŸ“ Structure du projet

```
QLearning-Mobile-GridWorld/
â”‚
â”œâ”€â”€ environment.py       # Classe MobileGridWorld
â”œâ”€â”€ agent.py            # Classe AdaptiveQLearning
â”œâ”€â”€ main.py             # Programme principal
â”œâ”€â”€ README.md           # Documentation
â”œâ”€â”€ requirements.txt    # DÃ©pendances
â””â”€â”€ .gitignore         # Fichiers Ã  ignorer
```

## ğŸ”§ ParamÃ¨tres configurables

### Environnement
- `grid_size` : Taille de la grille (ex: 5 pour 5x5)
- `start_pos` : Position initiale de l'agent
- `goal_positions` : Liste des positions des goals
- `obstacles` : Liste des positions des obstacles
- `goal_move_prob` : ProbabilitÃ© de mouvement des goals (0.0-1.0)
- `obstacle_move_prob` : ProbabilitÃ© de mouvement des obstacles (0.0-1.0)

### Agent Q-Learning
- `alpha` : Taux d'apprentissage (dÃ©faut: 0.3)
- `gamma` : Facteur de discount (dÃ©faut: 0.95)
- `epsilon` : Taux d'exploration initial (dÃ©faut: 1.0)
- `num_episodes` : Nombre d'Ã©pisodes d'entraÃ®nement (dÃ©faut: 2000)
- `max_steps_per_episode` : Limite de pas par Ã©pisode (dÃ©faut: 200)

## ğŸ“ˆ RÃ©sultats attendus

AprÃ¨s l'entraÃ®nement, l'agent devrait :
- âœ… Atteindre un taux de succÃ¨s > 70%
- âœ… RÃ©duire progressivement le nombre de pas nÃ©cessaires
- âœ… Ã‰viter les boucles infinies
- âœ… S'adapter aux mouvements des goals et obstacles

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
- Signaler des bugs
- Proposer de nouvelles fonctionnalitÃ©s
- AmÃ©liorer la documentation

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ‘¤ Auteur

Votre Nom - [Votre GitHub](https://github.com/votre-username)

## ğŸ™ Remerciements

- InspirÃ© par les principes du Reinforcement Learning
- BasÃ© sur l'algorithme Q-Learning de Watkins & Dayan (1992)

---

**Note** : Ce projet est Ã  but Ã©ducatif pour comprendre l'apprentissage par renforcement dans des environnements dynamiques.
