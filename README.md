# ğŸ¤– Mobile GridWorld - Projet d'Apprentissage par Renforcement

## ğŸ“‹ Description du Projet

Ce projet implÃ©mente deux approches d'apprentissage par renforcement pour rÃ©soudre un problÃ¨me de navigation dans une grille mobile avec obstacles et objectifs dynamiques :

1. **RL_MobileGrid** : Q-Learning classique avec table Q
2. **RL_MobileGridNeural** : Q-Learning avec approximation par rÃ©seau de neurones (Deep Q-Learning)

---

## ğŸ“ Structure du Projet

```
â”œâ”€â”€ RL_MobileGrid/              # Approche classique (Q-Learning)
â”‚   â”œâ”€â”€ agent.py                # Agent avec table Q
â”‚   â”œâ”€â”€ environment.py          # Environnement GridWorld
â”‚   â””â”€â”€ main.py                 # Programme principal
â”‚
â”œâ”€â”€ RL_MobileGridNeural/        # Approche Deep Learning
â”‚   â”œâ”€â”€ agent_neural.py         # Agent avec rÃ©seau de neurones
â”‚   â”œâ”€â”€ environment.py          # Environnement GridWorld
â”‚   â””â”€â”€ main_neural.py          # Programme principal
â”‚
â””â”€â”€ README.md                   # Ce fichier
```

---

## ğŸ”§ Installation et PrÃ©requis

### 1. **PrÃ©requis**
- Python 3.8 ou supÃ©rieur
- Visual Studio Code (recommandÃ©)

### 2. **Installation des dÃ©pendances**

Ouvrez un terminal dans le dossier du projet et exÃ©cutez :

```bash
pip install numpy matplotlib torch
```

**DÃ©tail des bibliothÃ¨ques :**
- `numpy` : Calculs numÃ©riques
- `matplotlib` : Visualisation graphique
- `torch` : PyTorch pour le rÃ©seau de neurones (uniquement pour RL_MobileGridNeural)

---

## ğŸš€ Utilisation

### **Option 1 : Q-Learning Classique** (Table Q)

#### Ã‰tapes d'exÃ©cution dans VS Code :

1. **Ouvrir le dossier** `RL_MobileGrid` dans VS Code
2. **Ouvrir le terminal** (Ctrl + Ã¹ ou Terminal â†’ Nouveau Terminal)
3. **ExÃ©cuter le programme** :
   ```bash
   python main.py
   ```

#### DÃ©roulement du programme :

```
========================================
CONFIGURATION DE L'ENVIRONNEMENT MOBILE
========================================

Voulez-vous:
  1. Entrer les paramÃ¨tres manuellement
  2. GÃ©nÃ©rer alÃ©atoirement
Votre choix (1 ou 2):
```

**âœ… Choix 1 : Configuration Manuelle**
- Vous devrez entrer :
  - Taille de la grille (ex: 5 pour 5Ã—5)
  - Position initiale de l'agent (x, y)
  - Nombre et positions des goals mobiles
  - ProbabilitÃ© de mouvement des goals (recommandÃ© : 0.3)
  - Nombre et positions des obstacles mobiles
  - ProbabilitÃ© de mouvement des obstacles (recommandÃ© : 0.2)

**âœ… Choix 2 : Configuration AlÃ©atoire**
- Le programme gÃ©nÃ¨re automatiquement :
  - Une grille de taille alÃ©atoire (6Ã—6 Ã  9Ã—9)
  - 1 Ã  3 goals mobiles
  - Plusieurs obstacles mobiles
  - ProbabilitÃ©s de mouvement optimales

**Ensuite :**
```
Nombre d'Ã©pisodes d'entraÃ®nement (recommandÃ©: 1000-3000):
```
- Entrez le nombre d'Ã©pisodes (ex: 2000)
- L'entraÃ®nement commence automatiquement
- Des statistiques s'affichent tous les 100 Ã©pisodes

**Ã€ la fin de l'entraÃ®nement :**
- Une fenÃªtre matplotlib s'ouvre avec 4 graphiques :
  - Ã‰volution des rÃ©compenses
  - Taux de succÃ¨s
  - DÃ©croissance de l'exploration (epsilon)
  - Distribution des rÃ©compenses

- Le programme propose ensuite :
  ```
  Voulez-vous voir la simulation graphique? (o/n):
  ```
  - Tapez `o` pour voir l'agent naviguer en temps rÃ©el
  - Tapez `n` pour passer

- Puis :
  ```
  Voulez-vous comparer avec une politique alÃ©atoire? (o/n):
  ```
  - Tapez `o` pour voir une comparaison de performance
  - Tapez `n` pour terminer

---

### **Option 2 : Deep Q-Learning** (RÃ©seau de Neurones)

#### Ã‰tapes d'exÃ©cution dans VS Code :

1. **Ouvrir le dossier** `RL_MobileGridNeural` dans VS Code
2. **Ouvrir le terminal** (Ctrl + Ã¹ ou Terminal â†’ Nouveau Terminal)
3. **ExÃ©cuter le programme** :
   ```bash
   python main_neural.py
   ```

#### DÃ©roulement du programme :

Le programme suit **exactement les mÃªmes Ã©tapes** que l'Option 1, mais avec des diffÃ©rences importantes :

**ğŸ”¹ Architecture du rÃ©seau de neurones :**
```
========================================
PHASE D'ENTRAÃNEMENT DU RÃ‰SEAU DE NEURONES
========================================
Architecture: 2 -> 64 -> 64 -> 4
Replay Buffer: 5000 transitions
```

**ğŸ”¹ DiffÃ©rences clÃ©s :**
- Utilise un **rÃ©seau de neurones** au lieu d'une table Q
- Emploie un **replay buffer** pour stabiliser l'apprentissage
- Convergence gÃ©nÃ©ralement **plus rapide** et **plus robuste**
- Meilleure gÃ©nÃ©ralisation Ã  de grandes grilles

**ğŸ”¹ Options de visualisation :**
AprÃ¨s l'entraÃ®nement, vous avez le choix :

1. **Voir la simulation graphique :**
   ```
   Voulez-vous voir la simulation graphique? (o/n):
   ```
   - `o` : Affiche l'agent en action avec :
     - **Vue gauche** : Environnement en temps rÃ©el
     - **Vue droite** : Q-Values et politique apprise
   - `n` : Passer directement Ã  la suite

2. **Comparer avec une politique alÃ©atoire :**
   ```
   Voulez-vous comparer avec une politique alÃ©atoire? (o/n):
   ```
   - `o` : Graphique comparatif des performances
   - `n` : Terminer le programme

---

## ğŸ“Š InterprÃ©tation des RÃ©sultats

### **Courbes d'apprentissage :**

1. **Ã‰volution des RÃ©compenses**
   - Doit augmenter au fil des Ã©pisodes
   - La moyenne mobile (rouge) doit converger vers une valeur positive

2. **Taux de SuccÃ¨s**
   - Doit tendre vers 100% (ou proche)
   - Indique le pourcentage d'Ã©pisodes oÃ¹ le goal est atteint

3. **DÃ©croissance d'Epsilon**
   - Montre la transition exploration â†’ exploitation
   - Doit diminuer progressivement de 1.0 vers 0.05

4. **Distribution des RÃ©compenses**
   - Doit montrer une concentration vers des valeurs Ã©levÃ©es
   - Peu de rÃ©compenses nÃ©gatives en fin d'entraÃ®nement

### **Simulation Graphique :**

**LÃ©gende :**
- ğŸ”µ **Point bleu** : Agent
- â­ **Ã‰toile rouge** : Goal non atteint
- â­ **Ã‰toile verte** : Goal atteint
- â¬› **CarrÃ© gris** : Obstacle mobile
- ğŸ¯ **FlÃ¨ches cyan** : Politique apprise (vue droite)
- ğŸŸ¢ **IntensitÃ© verte** : Q-Values moyennes (vue droite)

---

## âš™ï¸ ParamÃ¨tres RecommandÃ©s

### **Pour une grille 5Ã—5 :**
- Ã‰pisodes : 1000-2000
- Goals : 1-2
- Obstacles : 3-5
- Goal move prob : 0.3
- Obstacle move prob : 0.2

### **Pour une grille 8Ã—8 ou plus grande :**
- Ã‰pisodes : 3000-5000 (classique) ou 2000-3000 (neural)
- Goals : 2-3
- Obstacles : 8-15
- Goal move prob : 0.2-0.4
- Obstacle move prob : 0.1-0.3

---

## ğŸ¯ Objectifs PÃ©dagogiques

Ce projet permet de :
1. âœ… Comparer Q-Learning classique vs. Deep Q-Learning
2. âœ… Observer l'impact de l'exploration (epsilon-greedy)
3. âœ… Comprendre le replay buffer et la stabilisation
4. âœ… Visualiser l'apprentissage en temps rÃ©el
5. âœ… GÃ©rer des environnements dynamiques (goals et obstacles mobiles)

---

## ğŸ› DÃ©pannage

### **ProblÃ¨me : "ModuleNotFoundError: No module named 'torch'"**
```bash
pip install torch
```

### **ProblÃ¨me : Les fenÃªtres matplotlib ne s'affichent pas**
- VÃ©rifiez que matplotlib est installÃ© : `pip install matplotlib`
- Sur certains systÃ¨mes, ajoutez : `pip install PyQt5`

### **ProblÃ¨me : L'agent ne converge pas**
- Augmentez le nombre d'Ã©pisodes
- RÃ©duisez la probabilitÃ© de mouvement des obstacles
- Augmentez la taille de la grille si elle est trop petite

### **ProblÃ¨me : "RuntimeError: CUDA out of memory"** (Neural)
- RÃ©duisez la taille du replay buffer (dans `agent_neural.py`)
- Utilisez CPU au lieu de GPU (PyTorch le fait automatiquement si nÃ©cessaire)

---

## ğŸ“ Notes pour le Professeur

### **Temps d'exÃ©cution estimÃ© :**
- Configuration : 1-2 minutes
- EntraÃ®nement (1000 Ã©pisodes) : 2-5 minutes
- Simulation graphique : 30 secondes - 2 minutes

### **Points d'Ã©valuation suggÃ©rÃ©s :**
1. âœ… Convergence de l'algorithme
2. âœ… QualitÃ© de la politique apprise
3. âœ… Comparaison des deux approches
4. âœ… Gestion des environnements dynamiques
5. âœ… ClartÃ© du code et documentation

### **Extensions possibles :**
- Ajouter des rÃ©compenses intermÃ©diaires (shaped rewards)
- ImplÃ©menter Double DQN ou Dueling DQN
- Tester avec des grilles plus grandes (15Ã—15)
- Ajouter plusieurs types d'obstacles avec comportements diffÃ©rents

---

## ğŸ‘¥ Auteurs

- **Projet rÃ©alisÃ© dans le cadre du cours d'Apprentissage par Renforcement**
- **UniversitÃ© : [Votre UniversitÃ©]**
- **Date : Octobre 2025**

---

## ğŸ“„ Licence

Ce projet est Ã  usage Ã©ducatif uniquement.

---

## ğŸ“§ Contact

Pour toute question concernant ce projet, veuillez contacter :
- Email : zakariaezemmahi@gmail.com
- GitHub : zakariazemmahi

---

**Bon entraÃ®nement ! ğŸš€ğŸ¤–**
